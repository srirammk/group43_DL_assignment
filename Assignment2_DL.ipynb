{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Assignment2_DL.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nsgDx3cHIkt7"
      },
      "source": [
        "## **Import Libraries/Dataset**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "byoeMiXBHL7T"
      },
      "source": [
        "**Import packages**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6aCwq1yH20Re"
      },
      "source": [
        "import string\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from pickle import load,dump\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import sys, time, os, warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "import re\n",
        "\n",
        "import keras\n",
        "import tensorflow as tf\n",
        "from tqdm import tqdm\n",
        "from nltk.translate.bleu_score import sentence_bleu\n",
        "\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "#from tensorflow.keras.utils import to_categorical\n",
        "#from tensorflow.keras.utils import plot_model\n",
        "from keras.models import Model\n",
        "#from keras.layers import Input\n",
        "from keras.layers import Dense, BatchNormalization,GRU,Embedding,Dropout\n",
        "# from keras.layers import GRU\n",
        "# from keras.layers import Embedding\n",
        "# from keras.layers import Dropout\n",
        "#from keras.layers.merge import add\n",
        "#from keras.callbacks import ModelCheckpoint\n",
        "from keras.preprocessing.image import load_img\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.applications.vgg16 import VGG16, preprocess_input\n",
        "from keras.regularizers import l2\n",
        "\n",
        "from sklearn.utils import shuffle\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.utils import shuffle\n",
        "import os.path\n",
        "import zipfile"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "49p0ZYaoHZui"
      },
      "source": [
        "**Mounting the Google Drive to get and Store data**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Oq4XAvkd3Axg",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b1622d83-87a3-4f06-9e50-8119b193cdd0"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Lgb6OtnGX7pC"
      },
      "source": [
        "BASE_PATH = '/content/drive/MyDrive/group43/dl/assignment2/'\n",
        "IMAGES_FOLDER =  BASE_PATH + \"Flicker8k_Dataset/\"\n",
        "FEATURE_FILE = BASE_PATH + \"/data/features.pkl\"\n",
        "FEATURE_EXTRACTED = BASE_PATH + \"/data/saved/features.extracted.id\"\n",
        "IMAGE_SUFFIX= \".jpg\"\n",
        "IMAGE_SIZE = (299, 299)\n",
        "VOCAB_SIZE = 10000\n",
        "SEQ_LENGTH = 20\n",
        "EMBED_DIM = 512\n",
        "UNIT_DIM = 256\n",
        "NUM_HEADS = 2\n",
        "FF_DIM = 512\n",
        "BATCH_SIZE = 64\n",
        "EPOCHS = 30\n",
        "AUTOTUNE = tf.data.AUTOTUNE\n",
        "LEARNING_RATE = 0.00001"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GBimaOomHh60"
      },
      "source": [
        "**Importing Dataset**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LvOYb3Kw3LD4"
      },
      "source": [
        "jpgs = os.listdir(IMAGES_FOLDER)\n",
        "#fileName = \"set_3\"\n",
        "pickle_file = BASE_PATH+ \"set_3.pkl\"\n",
        "df = pd.read_pickle(pickle_file, compression='infer')\n",
        "print(\"Size of the dataset\",len(df))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LKfikEybI2x7"
      },
      "source": [
        "## **Data Visualization and augmentation**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5UqtBLzFHnZ-"
      },
      "source": [
        "**Loading the data**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ALZh6Ifc3UJ2"
      },
      "source": [
        "datatxt = []\n",
        "for row in df:\n",
        "   col = row.split('\\t')\n",
        "   if len(col) == 1:\n",
        "       continue\n",
        "   w = col[0].split(\"#\")\n",
        "   datatxt.append(w + [col[1].lower()])\n",
        "\n",
        "data = pd.DataFrame(datatxt,columns=[\"filename\",\"index\",\"caption\"])\n",
        "data = data.reindex(columns =['index','filename','caption'])\n",
        "\n",
        "data = data[data.filename != '2258277193_586949ec62.jpg.1']\n",
        "uni_filenames = np.unique(data.filename.values)\n",
        "\n",
        "data.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1dsxqcrHVblS"
      },
      "source": [
        "**Plot of two samples and their captions**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8N7BEKm14XPF"
      },
      "source": [
        "npic = 5\n",
        "npix = 224\n",
        "target_size = (npix,npix,3)\n",
        "count = 1\n",
        "\n",
        "fig = plt.figure(figsize=(10,20))\n",
        "for jpgfnm in uni_filenames[12:14]:\n",
        "   filename = IMAGES_FOLDER + '/' + jpgfnm\n",
        "   captions = list(data[\"caption\"].loc[data[\"filename\"]==jpgfnm].values)\n",
        "   image_load = load_img(filename, target_size=target_size)\n",
        "   ax = fig.add_subplot(npic,2,count,xticks=[],yticks=[])\n",
        "   ax.imshow(image_load)\n",
        "   count += 1\n",
        "\n",
        "   ax = fig.add_subplot(npic,2,count)\n",
        "   plt.axis('off')\n",
        "   ax.plot()\n",
        "   ax.set_xlim(0,1)\n",
        "   ax.set_ylim(0,len(captions))\n",
        "   for i, caption in enumerate(captions):\n",
        "       ax.text(0,i,caption,fontsize=20,color=\"blue\")\n",
        "   count += 1\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XmoRUzQAIEtl"
      },
      "source": [
        "**Preprocessing the data**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G8_G5r0i5frg"
      },
      "source": [
        "vocabulary = []\n",
        "for txt in data.caption.values:\n",
        "   vocabulary.extend(txt.split())\n",
        "print('Vocabulary Size: %d' % len(set(vocabulary)))\n",
        "\n",
        "def remove_punctuation(text_original):\n",
        "   text_no_punctuation = text_original.translate(string.punctuation)\n",
        "   return(text_no_punctuation)\n",
        "\n",
        "def remove_single_character(text):\n",
        "   text_len_more_than1 = \"\"\n",
        "   for word in text.split():\n",
        "       if len(word) > 1:\n",
        "           text_len_more_than1 += \" \" + word\n",
        "   return(text_len_more_than1)\n",
        "\n",
        "def remove_numeric(text):\n",
        "   text_no_numeric = \"\"\n",
        "   for word in text.split():\n",
        "       isalpha = word.isalpha()\n",
        "       if isalpha:\n",
        "           text_no_numeric += \" \" + word\n",
        "   return(text_no_numeric)\n",
        "\n",
        "def text_clean(text_original):\n",
        "   text = remove_punctuation(text_original)\n",
        "   text = remove_single_character(text)\n",
        "   text = remove_numeric(text)\n",
        "   return(text)\n",
        "\n",
        "for i, caption in enumerate(data.caption.values):\n",
        "   newcaption = text_clean(caption)\n",
        "   data[\"caption\"].iloc[i] = newcaption\n",
        "\n",
        "clean_vocabulary = []\n",
        "for txt in data.caption.values:\n",
        "   clean_vocabulary.extend(txt.split())\n",
        "print('Clean Vocabulary Size: %d' % len(set(clean_vocabulary)))\n",
        "\n",
        "all_captions = []\n",
        "for caption  in data[\"caption\"].astype(str):\n",
        "   caption = '<start> ' + caption+ ' <end>'\n",
        "   all_captions.append(caption)\n",
        "\n",
        "all_captions[:10]\n",
        "\n",
        "\n",
        "all_img_name_vector = []\n",
        "for annot in data[\"filename\"]:\n",
        "   full_image_path = IMAGES_FOLDER + annot\n",
        "   all_img_name_vector.append(full_image_path)\n",
        "\n",
        "all_img_name_vector[:10]\n",
        "\n",
        "words = [txt.split() for txt in data[\"caption\"].astype(str)]\n",
        "\n",
        "unique = []\n",
        "for word in words:\n",
        "    unique.extend(word)\n",
        "unique = list(set(unique))\n",
        "word_index = {}\n",
        "index_word={}\n",
        "\n",
        "for i,word in enumerate(unique):\n",
        "\n",
        "    word_index[word] = i\n",
        "    index_word[i] = word\n",
        "\n",
        "partial_captions = []\n",
        "for text in data[\"caption\"].astype(str):\n",
        "  one = [word_index[txt] for txt in text.split()]\n",
        "  partial_captions.append(one)\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x0KwGyTi6qYA"
      },
      "source": [
        "def data_limiter(num,total_captions,all_img_name_vector):\n",
        " train_captions, img_name_vector = shuffle(total_captions,all_img_name_vector,random_state=1)\n",
        " train_captions = train_captions[:num]\n",
        " img_name_vector = img_name_vector[:num]\n",
        " return train_captions,img_name_vector\n",
        "\n",
        "train_captions,img_name_vector = data_limiter(8000,all_captions,all_img_name_vector)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RM114Zq4JS4D"
      },
      "source": [
        "## **Model Building**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aymTQFwhJc7y"
      },
      "source": [
        "**Use Pretrained VGG-16 model trained on ImageNet dataset for image feature extraction.**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-sZQZEkC60TS"
      },
      "source": [
        "def load_image(image_path):\n",
        "   img = tf.io.read_file(image_path)\n",
        "   img = tf.image.decode_jpeg(img, channels=3)\n",
        "   img = tf.image.resize(img, (224, 224))\n",
        "   img = preprocess_input(img)\n",
        "   return img, image_path\n",
        "\n",
        "image_model = tf.keras.applications.VGG16(include_top=False, weights='imagenet')\n",
        "new_input = image_model.input\n",
        "hidden_layer = image_model.layers[-1].output\n",
        "image_features_extract_model = Model(new_input, hidden_layer)\n",
        "\n",
        "image_features_extract_model.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S8ojS-ch66dO"
      },
      "source": [
        "encode_train = sorted(set(img_name_vector))\n",
        "image_dataset = tf.data.Dataset.from_tensor_slices(encode_train)\n",
        "image_dataset = image_dataset.map(load_image, num_parallel_calls=tf.data.experimental.AUTOTUNE).batch(64)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yJdTm3Dg9nRw"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OIuziY_C6_M8"
      },
      "source": [
        "%%time\n",
        "if not os.path.isfile(FEATURE_EXTRACTED):\n",
        "\n",
        "  for img, path in tqdm(image_dataset):\n",
        "    batch_features = image_features_extract_model(img)\n",
        "    batch_features = tf.reshape(batch_features,\n",
        "                              (batch_features.shape[0], -1, batch_features.shape[3]))\n",
        "\n",
        "    for bf, p in zip(batch_features, path):\n",
        "      path_of_feature = p.numpy().decode(\"utf-8\")\n",
        "      #all_features[path_of_feature] =bf.numpy()\n",
        "      fileNamePath= BASE_PATH+ \"data/saved/\"+ os.path.basename(path_of_feature)\n",
        "      np.save(fileNamePath, bf.numpy())\n",
        "  f = open(FEATURE_EXTRACTED, \"w\")\n",
        "  f.write(\"Feature Extracted\")\n",
        "  f.close()\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YHGnSiDIJq2C"
      },
      "source": [
        "**Tokenization of data**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vN4ihIqT7GQ4"
      },
      "source": [
        "tokenizer = tf.keras.preprocessing.text.Tokenizer(\n",
        "                                                 oov_token=\"<unk>\",\n",
        "                                                 filters='!\"#$%&()*+.,-/:;=?@[\\]^_`{|}~ ')\n",
        "\n",
        "tokenizer.fit_on_texts(train_captions)\n",
        "train_seqs = tokenizer.texts_to_sequences(train_captions)\n",
        "tokenizer.word_index['<pad>'] = 0\n",
        "tokenizer.index_word[0] = '<pad>'\n",
        "train_seqs = tokenizer.texts_to_sequences(train_captions)\n",
        "cap_vector = tf.keras.preprocessing.sequence.pad_sequences(train_seqs, padding='post')\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zaOq7QSXFl1S"
      },
      "source": [
        "def calc_max_length(tensor):\n",
        "   return max(len(t) for t in tensor)\n",
        "max_length = calc_max_length(train_seqs)\n",
        "\n",
        "def calc_min_length(tensor):\n",
        "   return min(len(t) for t in tensor)\n",
        "min_length = calc_min_length(train_seqs)\n",
        "\n",
        "print('Max Length of any caption : Min Length of any caption = '+ str(max_length) +\" : \"+str(min_length))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A-TJf59dFr2d"
      },
      "source": [
        "img_name_train, img_name_val, cap_train, cap_val = train_test_split(img_name_vector,cap_vector, test_size=0.2, random_state=0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oJffabb0Fw5i"
      },
      "source": [
        "BATCH_SIZE = 64\n",
        "BUFFER_SIZE = 1000\n",
        "embedding_dim = 256\n",
        "units = 512\n",
        "vocab_size = len(tokenizer.word_index) + 1\n",
        "num_steps = len(img_name_train) // BATCH_SIZE\n",
        "features_shape = 512\n",
        "attention_features_shape = 49"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J35Ygh_0F162"
      },
      "source": [
        "def map_func(img_name, cap):\n",
        " fileNamePath= BASE_PATH+ \"data/saved/\"+ os.path.basename(img_name.decode('utf-8'))\n",
        "\n",
        " img_tensor = np.load(fileNamePath+'.npy')\n",
        " #img_tensor = all_features[img_name.decode('utf-8')]\n",
        " return img_tensor, cap\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UTPosaMOKxfg"
      },
      "source": [
        "train_dataset = tf.data.Dataset.from_tensor_slices((img_name_train, cap_train))\n",
        "# Use map to load the numpy files in parallel\n",
        "train_dataset = train_dataset.map(lambda item1, item2: tf.numpy_function(\n",
        "        map_func, [item1, item2], [tf.float32, tf.int32]),\n",
        "         num_parallel_calls=tf.data.experimental.AUTOTUNE)\n",
        "\n",
        "train_dataset = train_dataset.shuffle(BUFFER_SIZE).batch(BATCH_SIZE)\n",
        "train_dataset = train_dataset.prefetch(buffer_size=tf.data.experimental.AUTOTUNE)\n",
        "\n",
        "\n",
        "# test_dataset = tf.data.Dataset.from_tensor_slices((img_name_val, cap_val))\n",
        "# # Use map to load the numpy files in parallel\n",
        "# test_dataset = test_dataset.map(lambda item1, item2: tf.numpy_function(\n",
        "#         map_func, [item1, item2], [tf.float32, tf.int32]),\n",
        "#          num_parallel_calls=tf.data.experimental.AUTOTUNE)\n",
        "\n",
        "# test_dataset = test_dataset.shuffle(BUFFER_SIZE).batch(BATCH_SIZE)\n",
        "# test_dataset = test_dataset.prefetch(buffer_size=tf.data.experimental.AUTOTUNE)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5a07ruHeF7O1"
      },
      "source": [
        "class Encoder(tf.keras.Model):\n",
        "   # This encoder passes the features through a Fully connected layer\n",
        "   def __init__(self, embedding_dim):\n",
        "       super(Encoder, self).__init__()\n",
        "       # shape after fc == (batch_size, 49, embedding_dim)\n",
        "       self.fc = tf.keras.layers.Dense(embedding_dim)\n",
        "       self.dropout = tf.keras.layers.Dropout(0.5, noise_shape=None, seed=None)\n",
        "\n",
        "   def call(self, x):\n",
        "       #x= self.dropout(x)\n",
        "       x = self.fc(x)\n",
        "       x = tf.nn.relu(x)\n",
        "       return x  \n",
        "\n",
        "class RNN_Decoder(tf.keras.Model):\n",
        " def __init__(self, embedding_dim, units, vocab_size):\n",
        "    super(RNN_Decoder, self).__init__()\n",
        "    self.units = units\n",
        "    self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n",
        "    self.gru_1 = GRU(self.units,\n",
        "                                  return_sequences=True,\n",
        "                                  return_state=True,\n",
        "                                  recurrent_initializer='glorot_uniform',\n",
        "                                  kernel_regularizer=l2(0.01), \n",
        "                                  bias_regularizer=l2(0.01))\n",
        "    self.gru_2 = GRU(self.units,\n",
        "                                  return_sequences=True,\n",
        "                                  return_state=True,\n",
        "                                  recurrent_initializer='glorot_uniform')\n",
        "    self.gru_3 = GRU(self.units,\n",
        "                                  return_sequences=True,\n",
        "                                  return_state=True,\n",
        "                                  recurrent_initializer='glorot_uniform') \n",
        "    self.fc1 = Dense(self.units)\n",
        "\n",
        "    self.dropout = Dropout(0.5, noise_shape=None, seed=None)\n",
        "    self.batchnormalization = BatchNormalization(axis=-1, momentum=0.99, \n",
        "                                  epsilon=0.001, center=True, scale=True, beta_initializer='zeros', \n",
        "                                  gamma_initializer='ones', moving_mean_initializer='zeros', \n",
        "                                  moving_variance_initializer='ones', beta_regularizer=None, gamma_regularizer=None, \n",
        "                                  beta_constraint=None, gamma_constraint=None)\n",
        "\n",
        "    self.fc2 = Dense(vocab_size)\n",
        "\n",
        "    # Implementing Attention Mechanism\n",
        "    self.Uattn = Dense(units)\n",
        "    self.Wattn = Dense(units)\n",
        "    self.Vattn = Dense(1)\n",
        "\n",
        " def call(self, x, features, hidden):\n",
        "\n",
        "   hidden_with_time_axis = tf.expand_dims(hidden, 1)\n",
        "\n",
        "   score = self.Vattn(tf.nn.tanh(self.Uattn(features) + self.Wattn(hidden_with_time_axis)))\n",
        "\n",
        "   # you get 1 at the last axis because you are applying score to self.Vattn\n",
        "   # Then find Probability using Softmax\n",
        "   '''attention_weights(alpha(ij)) = softmax(e(ij))'''\n",
        "\n",
        "   attention_weights = tf.nn.softmax(score, axis=1)\n",
        "\n",
        "   # attention_weights shape == (64, 49, 1)\n",
        "   # Give weights to the different pixels in the image\n",
        "   ''' C(t) = Summation(j=1 to T) (attention_weights * VGG-16 features) '''\n",
        "\n",
        "   context_vector = attention_weights * features\n",
        "   context_vector = tf.reduce_sum(context_vector, axis=1)\n",
        "\n",
        "   # Context Vector(64,256) = AttentionWeights(64,49,1) * features(64,49,256)\n",
        "   # context_vector shape after sum == (64, 256)\n",
        "   # x shape after passing through embedding == (64, 1, 256)\n",
        "\n",
        "   x = self.embedding(x)\n",
        "   # x shape after concatenation == (64, 1,  512)\n",
        "\n",
        "   x = tf.concat([tf.expand_dims(context_vector, 1), x], axis=-1)\n",
        "   # passing the concatenated vector to the GRU\n",
        "\n",
        "   output,state = self.gru_1(x)\n",
        "   output,state = self.gru_2(output,state)\n",
        "   output,state = self.gru_3(output,state)\n",
        "\n",
        "   x = self.fc1(output)\n",
        "\n",
        "   # x shape == (batch_size * max_length, hidden_size)\n",
        "   x = tf.reshape(x, (-1, x.shape[2]))\n",
        "\n",
        "   # Adding Dropout and BatchNorm Layers\n",
        "   x= self.dropout(x)\n",
        "   x= self.batchnormalization(x)\n",
        "\n",
        "   # output shape == (64 * 512)\n",
        "   x = self.fc2(x)\n",
        "\n",
        "   # shape : (64 * 8329(vocab))\n",
        "   return x, state, attention_weights\n",
        "\n",
        " def reset_state(self, batch_size):\n",
        "   return tf.zeros((batch_size, self.units))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ME6vgsLpGEWS"
      },
      "source": [
        "encoder = Encoder(embedding_dim)\n",
        "decoder = RNN_Decoder(embedding_dim, units, vocab_size)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3oGGDv3mKDSz"
      },
      "source": [
        "optimizer = tf.keras.optimizers.Adam(learning_rate=LEARNING_RATE)\n",
        "loss_object = tf.keras.losses.SparseCategoricalCrossentropy(\n",
        "   from_logits=True, reduction='none')\n",
        "\n",
        "train_loss = tf.keras.metrics.Mean(name='train_loss')\n",
        "train_accuracy = tf.keras.metrics.SparseCategoricalAccuracy(name='train_accuracy')\n",
        "\n",
        "# test_loss = tf.keras.metrics.Mean(name='test_loss')\n",
        "# test_accuracy = tf.keras.metrics.SparseCategoricalAccuracy(name='test_accuracy')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YXkUWh7SKou8"
      },
      "source": [
        "## **Model Training**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y2yzW-SXGiHX"
      },
      "source": [
        "train_loss_plot = []\n",
        "test_loss_plot = []\n",
        "train_accuracy_plot=[]\n",
        "test_accuracy_plot=[]\n",
        "\n",
        "@tf.function\n",
        "def train_step(img_tensor, target):\n",
        " # initializing the hidden state for each batch\n",
        " # because the captions are not related from image to image\n",
        "\n",
        " hidden = decoder.reset_state(batch_size=target.shape[0])\n",
        " dec_input = tf.expand_dims([tokenizer.word_index['<start>']] * BATCH_SIZE, 1)\n",
        "\n",
        " with tf.GradientTape() as tape:\n",
        "     features = encoder(img_tensor)\n",
        "     for i in range(1, target.shape[1]):\n",
        "         # passing the features through the decoder\n",
        "         predictions, hidden, _ = decoder(dec_input, features, hidden)\n",
        "         loss = loss_object(target[:, i], predictions)\n",
        "         train_loss(loss)\n",
        "         train_accuracy(target[:, i], predictions)\n",
        "         # using teacher forcing\n",
        "         dec_input = tf.expand_dims(target[:, i], 1)\n",
        "\n",
        " trainable_variables = encoder.trainable_variables + decoder.trainable_variables\n",
        " gradients = tape.gradient(loss, trainable_variables)\n",
        " optimizer.apply_gradients(zip(gradients, trainable_variables))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zrs84y23JSJD"
      },
      "source": [
        "# @tf.function\n",
        "# def test_step(images, labels):\n",
        "#   # training=False is only needed if there are layers with different\n",
        "#   # behavior during training versus inference (e.g. Dropout).\n",
        "#   hidden = decoder.reset_state(batch_size=target.shape[0])\n",
        "#   dec_input = tf.expand_dims([tokenizer.word_index['<start>']] * BATCH_SIZE, 1)\n",
        "\n",
        "#   with tf.GradientTape() as tape:\n",
        "#      features = encoder(img_tensor)\n",
        "#      for i in range(1, target.shape[1]):\n",
        "#          # passing the features through the decoder\n",
        "#          predictions, hidden, _ = decoder(dec_input, features, hidden)\n",
        "#          loss = loss_object(target[:, i], predictions)\n",
        "#          test_loss(loss)\n",
        "#          test_accuracy(target[:, i], predictions)\n",
        "#          # using teacher forcing\n",
        "#          dec_input = tf.expand_dims(target[:, i], 1)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q9Bg3xe5GpFn"
      },
      "source": [
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "for epoch in range(1, EPOCHS):\n",
        "  train_loss.reset_states()\n",
        "  train_accuracy.reset_states()\n",
        "  test_loss.reset_states()\n",
        "  test_accuracy.reset_states()\n",
        "  start = time.time()\n",
        "  total_loss = 0\n",
        "  total_accuracy=0\n",
        "  for (batch, (img_tensor, target)) in enumerate(train_dataset):\n",
        "      #batch_loss, t_loss,batch_accuracy,t_accuracy = train_step(img_tensor, target)\n",
        "      train_step(img_tensor, target)\n",
        "\n",
        "  # for (batch,(img_tensor, target)) in enumerate(test_dataset):\n",
        "  #     test_step(img_tensor,target)\n",
        "  \n",
        "  train_loss_plot.append(train_loss.result().numpy()) \n",
        "  train_accuracy_plot.append(train_accuracy.result().numpy()*100)   \n",
        "  # test_loss_plot.append(test_loss.result().numpy()) \n",
        "  # test_accuracy_plot.append(test_accuracy.result().numpy()*100)   \n",
        "\n",
        "  print(\n",
        "  f'Epoch {epoch + 1}, '\n",
        "  f'Loss: {train_loss.result()}, '\n",
        "  f'Accuracy: {train_accuracy.result() * 100} '\n",
        "  # f'Test Loss: {test_loss.result()}, '\n",
        "  # f'Test Accuracy: {test_accuracy.result() * 100}'\n",
        "  )\n",
        "  print ('Time taken for 1 epoch {} sec\\n'.format(time.time() - start))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EfkV5wIUHBpu"
      },
      "source": [
        "\n",
        "# plt.plot(train_loss_plot)\n",
        "# plt.xlabel('Epochs')\n",
        "# plt.ylabel('Loss')\n",
        "# plt.title('Loss Plot')\n",
        "# plt.show()\n",
        "\n",
        "# plot the losses\n",
        "plt.style.use(\"ggplot\")\n",
        "plt.figure()\n",
        "plt.plot(np.arange(0, len(train_loss_plot)), train_loss_plot,\n",
        "\tlabel=\"Training Loss\")\n",
        "plt.plot(np.arange(0, len(train_accuracy_plot)), train_accuracy_plot,\n",
        "\tlabel=\"Training Accuracy\")\n",
        "plt.xlabel(\"Epoch #\")\n",
        "plt.ylabel(\"Loss\")\n",
        "plt.title('Training Accuracy Loss Plot Training')\n",
        "plt.legend(loc=\"upper right\")\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g6ToT4Akt3wr"
      },
      "source": [
        "### **Model Evaluation**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hhPR9vnEt0Js"
      },
      "source": [
        "def evaluate(image):\n",
        "  attention_plot = np.zeros((max_length, attention_features_shape))\n",
        "\n",
        "  hidden = decoder.reset_state(batch_size=1)\n",
        "  temp_input = tf.expand_dims(load_image(image)[0], 0)\n",
        "  img_tensor_val = image_features_extract_model(temp_input)\n",
        "  img_tensor_val = tf.reshape(img_tensor_val, (img_tensor_val.shape[0], -1, img_tensor_val.shape[3]))\n",
        "\n",
        "  extracted_features = encoder(img_tensor_val)\n",
        "  dec_input = tf.expand_dims([tokenizer.word_index['<start>']], 0)\n",
        "  result = []\n",
        "\n",
        "  for i in range(max_length):\n",
        "      predictions, hidden, attention_weights = decoder(dec_input, extracted_features, hidden)\n",
        "      attention_plot[i] = tf.reshape(attention_weights, (-1, )).numpy()\n",
        "      predicted_id = tf.argmax(predictions[0]).numpy()\n",
        "      result.append(tokenizer.index_word[predicted_id])\n",
        "\n",
        "      if tokenizer.index_word[predicted_id] == '<end>':\n",
        "          return result, attention_plot\n",
        "\n",
        "      dec_input = tf.expand_dims([predicted_id], 0)\n",
        "  attention_plot = attention_plot[:len(result), :]\n",
        "\n",
        "  return result, attention_plot"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XfVWaPNYt_iC"
      },
      "source": [
        "def plot_attention(image, result, attention_plot):\n",
        "   temp_image = np.array(Image.open(image))\n",
        "   fig = plt.figure(figsize=(10, 10))\n",
        "   len_result = len(result)\n",
        "   for l in range(len_result):\n",
        "       temp_att = np.resize(attention_plot[l], (8, 8))\n",
        "       ax = fig.add_subplot(len_result//2, len_result//2, l+1)\n",
        "       ax.set_title(result[l])\n",
        "       img = ax.imshow(temp_image)\n",
        "       ax.imshow(temp_att, cmap='gray', alpha=0.6, extent=img.get_extent())\n",
        "\n",
        "   plt.tight_layout()\n",
        "   plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CfJY50AFuEJn"
      },
      "source": [
        "# captions on the validation set\n",
        "rid = np.random.randint(0, len(img_name_val))\n",
        "image = IMAGES_FOLDER +'2319175397_3e586cfaf8.jpg'\n",
        "\n",
        "# real_caption = ' '.join([tokenizer.index_word[i] for i in cap_val[rid] if i not in [0]])\n",
        "result, attention_plot = evaluate(image)\n",
        "real_caption = 'Two white dogs are playing in the snow'\n",
        "# remove <start> and <end> from the real_caption\n",
        "first = real_caption.split(' ', 1)[1]\n",
        "\n",
        "\n",
        "#remove \"<unk>\" in result\n",
        "for i in result:\n",
        "   if i==\"<unk>\":\n",
        "       result.remove(i)\n",
        "\n",
        "for i in real_caption:\n",
        "   if i==\"<unk>\":\n",
        "       real_caption.remove(i)\n",
        "\n",
        "#remove <end> from result        \n",
        "result_join = ' '.join(result)\n",
        "result_final = result_join.rsplit(' ', 1)[0]\n",
        "\n",
        "real_appn = []\n",
        "real_appn.append(real_caption.split())\n",
        "reference = real_appn\n",
        "candidate = result\n",
        "\n",
        "score = sentence_bleu(reference, candidate)\n",
        "print(f\"BELU score: {score*100}\")\n",
        "\n",
        "print ('Real Caption:', real_caption)\n",
        "print ('Prediction Caption:', result_final)\n",
        "plot_attention(image, result, attention_plot)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}